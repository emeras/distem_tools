## How to prepare a Charm++ experiment on G5K.

### Connect the platform, then nancy site (scripts and images are there)
ssh access.grid5000.fr -l <G5K_USER_NAME>
ssh nancy

### If not already done generate ssh keys for nopasswd access between nodes
ssh-keygen -t rsa
cat .ssh/id_rsa.pub >> .ssh/authorized_keys

### Reserve some nodes
# Note: it is possible to reserve different IP range: e.g. /16=65,534 IP, /18=16,382 IP, /22=1,022 IP 
JOB=`oarsub -t deploy -l slash_18=1+cluster=1/nodes=10,walltime="04:00:00" 'sleep 99999999' 2>&1 | awk -F 'OAR_JOB_ID=' '{print $2}' | tail -n 1`

### Wait for the job to be running (State = R)
oarstat -j $JOB

### Then, connect to the job
oarsub -C $JOB

### Prepare experiment, first parameter is number of vnodes (virtual nodes) per host, second is vcore per vnode.
~jemeras/public/distem/distem_tools/prepare.sh 4 1

### Optional: Run test experiment (stencil3D)
~jemeras/public/distem/distem_tools/stencil_run.sh <TOTAL_NB_CORES>




############################
### Other types of tests ###
############################

### Distem scalability tests
# On first node (i.e. $SERVER), do:
cp ~jemeras/public/distem/distem_tools/collective_ops.c /root/
cd /root/ ; mpicc -O3 collective_ops.c -o collective_ops
#taktuk -s -f /tmp/distem_vnodes_ip broadcast put [ /root/collective_ops ] [ /root/collective_ops ]
for i in `cat /root/DISTEM_NODES`; do scp -p collective_ops $i:/tmp/distem/rootfs-shared/*/root; done
taktuk -s -f /tmp/distem_vnodes_ip broadcast exec [ hostname ]
# Then run mpi
rm run_times.log
for i in {1..10}; do
  /usr/bin/time -f %e --output=run_times.log --append mpirun -machinefile /tmp/distem_vnodes_ip --mca btl tcp,self ./collective_ops; 
done
# Or:
#rm -f /home/jemeras/public/expe.log || true
for t in {1..100}; do for i in 100 500 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000; do head -n $i /tmp/distem_vnodes_ip > out; /usr/bin/time -f %e --output=/home/jemeras/public/expe.log --append mpirun --mca btl tcp,self -machinefile out ./collective_ops; done >> /home/jemeras/public/expe.log; done


# clustershell
clush -w node[1-10000] true
